{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3423ca0f",
   "metadata": {},
   "source": [
    "Part 1: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec86fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import duckdb\n",
    "\n",
    "# Define URLs for required files\n",
    "taxi_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "zone_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "\n",
    "# Create data/raw directory if it doesn't exist\n",
    "data_dir = Path(\"data/raw\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Defines File paths for downloaded data\n",
    "taxi_path = data_dir / \"yellow_tripdata_2024-01.parquet\"\n",
    "zone_path = data_dir / \"taxi_zone_lookup.csv\"\n",
    "\n",
    "# Download Files and write to specified paths\n",
    "def download_file(url, path):\n",
    "     if path.exists():\n",
    "        return\n",
    "     \n",
    "     with requests.get(url, stream=True, timeout=30) as r:\n",
    "         r.raise_for_status()\n",
    "         with open(path, \"wb\") as f:\n",
    "             for chunk in r.iter_content(chunk_size=8192):\n",
    "                 f.write(chunk)\n",
    "\n",
    "download_file(taxi_url, taxi_path)\n",
    "download_file(zone_url, zone_path)\n",
    "print(\"\\nFiles downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d042768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Summary ===\n",
      "Total rows: 2,964,624\n",
      "Shape: (2964624, 10)\n",
      "\n",
      "Data Validated Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define expected columns\n",
    "expected_columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \"DOLocationID\", \n",
    "           \"passenger_count\", \"trip_distance\", \"fare_amount\", \"tip_amount\", \"total_amount\",\n",
    "           \"payment_type\"]\n",
    "\n",
    "datetime_columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "\n",
    "# Load Data with Polars\n",
    "df = pl.read_parquet(taxi_path, columns=expected_columns)\n",
    "\n",
    "def validate_data(df):\n",
    "    # Check for missing columns\n",
    "    missing_cols = set(expected_columns) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise Exception(f\"Missing expected columns: {missing_cols}\")\n",
    "\n",
    "    # Validate datetime columns\n",
    "    for col in datetime_columns:\n",
    "        if not df[col].dtype == pl.Datetime:\n",
    "            try:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Datetime))\n",
    "            except Exception:\n",
    "                raise Exception(f\"Invalid datetime values detected in column: {col}\")\n",
    "    return df\n",
    "\n",
    "# Print Row Count and Summary\n",
    "def print_summary(df):\n",
    "    print(\"\\n=== Dataset Summary ===\")\n",
    "    print(f\"Total rows: {len(df):,}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nData Validated Successfully!\")\n",
    "\n",
    "df = validate_data(df)\n",
    "print_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6e450",
   "metadata": {},
   "source": [
    "Part 2: Data Transformation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bc638c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaned Dataset Summary ===\n",
      "Total rows removed: 94,522\n",
      "Removed null values: 0\n",
      "Removed invalid distances: 60,371\n",
      "Removed negative fares: 34,065\n",
      "Removed exceeding $500: 30\n",
      "Removed invalid times: 56\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with nulls\n",
    "def remove_nulls(df):\n",
    "    num_rows = df.height\n",
    "\n",
    "    critical_columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \n",
    "                    \"DOLocationID\", \"fare_amount\"]\n",
    "    \n",
    "    df = df.drop_nulls(critical_columns)\n",
    "\n",
    "    removed_nulls = num_rows - df.height\n",
    "    return df, removed_nulls\n",
    "\n",
    "# Filter out invalid trips tracking reasons for removal\n",
    "def filter_trips(df):\n",
    "    current_rows = df.height\n",
    "\n",
    "    df = df.filter(pl.col(\"trip_distance\") > 0)\n",
    "    invalid_distance = current_rows - df.height\n",
    "    current_rows = df.height\n",
    "\n",
    "    df = df.filter(pl.col(\"fare_amount\") >= 0)\n",
    "    negative_fare = current_rows - df.height\n",
    "    current_rows = df.height\n",
    "\n",
    "    df = df.filter(pl.col(\"fare_amount\") <= 500)\n",
    "    exceeding_max = current_rows - df.height\n",
    "\n",
    "    return df, invalid_distance, negative_fare, exceeding_max\n",
    "\n",
    "# Filter out trips with dropoff before pickup\n",
    "def filter_time(df):\n",
    "    num_rows = df.height\n",
    "\n",
    "    df = df.filter(pl.col(\"tpep_dropoff_datetime\") >= pl.col(\"tpep_pickup_datetime\"))\n",
    "\n",
    "    removed_time = num_rows - df.height\n",
    "    return df, removed_time\n",
    "\n",
    "# Print summary of removals\n",
    "def save_and_print(df, total_removed, removed_nulls, invalid_distance, negative_fare, exceeding_max, removed_time):\n",
    "    print(\"\\n=== Cleaned Dataset Summary ===\")\n",
    "    print(f\"Total rows removed: {total_removed:,}\")\n",
    "    print(f\"Removed null values: {removed_nulls:,}\")\n",
    "    print(f\"Removed invalid distances: {invalid_distance:,}\")\n",
    "    print(f\"Removed negative fares: {negative_fare:,}\")\n",
    "    print(f\"Removed exceeding $500: {exceeding_max:,}\")\n",
    "    print(f\"Removed invalid times: {removed_time:,}\")\n",
    "\n",
    "original_rows = df.height\n",
    "\n",
    "df, removed_nulls = remove_nulls(df)\n",
    "df, invalid_distance, negative_fare, exceeding_max = filter_trips(df)\n",
    "df, removed_time = filter_time(df)\n",
    "\n",
    "total_removed = original_rows - df.height\n",
    "\n",
    "save_and_print(df, total_removed, removed_nulls, invalid_distance, negative_fare, exceeding_max, removed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5525e",
   "metadata": {},
   "source": [
    "From the data cleaning summary above, it can be seen that there contained no null values in the critical columns of the dataset however, invalid distances occupied around 63.9% of rows removed from the dataset. The second-most error prone column was the fare amount where 36.1% of rows were removed due to mostly negative amounts with a samll number of amounts exceeding $500. Finally, there existed only 56 out of the total 94,522 rows that were removed due to invalid pickup and dropoff times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844bfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Derived columns created successfully!\n",
      "\n",
      "Schema([('tpep_pickup_datetime', Datetime(time_unit='ns', time_zone=None)), ('tpep_dropoff_datetime', Datetime(time_unit='ns', time_zone=None)), ('PULocationID', Int32), ('DOLocationID', Int32), ('passenger_count', Int64), ('trip_distance', Float64), ('fare_amount', Float64), ('tip_amount', Float64), ('total_amount', Float64), ('payment_type', Int64), ('trip_duration_minutes', Float64), ('pickup_hour', Int8), ('pickup_day_of_week', Int8), ('trip_speed_mph', Float64)])\n"
     ]
    }
   ],
   "source": [
    "# Create derived columns for trip duration, pickup hour, day of week, and trip speed\n",
    "def create_derived_columns(df):\n",
    "    df = df.with_columns([\n",
    "        ((pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\")).dt.total_seconds() / 60)\n",
    "        .alias(\"trip_duration_minutes\"),\n",
    "\n",
    "        pl.col(\"tpep_pickup_datetime\").dt.hour().alias(\"pickup_hour\"),\n",
    "\n",
    "        pl.col(\"tpep_pickup_datetime\").dt.weekday().alias(\"pickup_day_of_week\"),\n",
    "    ]).with_columns([\n",
    "        (pl.when(pl.col(\"trip_duration_minutes\") > 0)\n",
    "         .then(pl.col(\"trip_distance\") / (pl.col(\"trip_duration_minutes\") / 60))\n",
    "         .otherwise(0)\n",
    "        ).alias(\"trip_speed_mph\"),\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "        \n",
    "df = create_derived_columns(df)\n",
    "print(\"\\nDerived columns created successfully!\\n\")\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d475793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data/clean directory and define output path\n",
    "data_dir = Path(\"data/clean\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = data_dir / \"yellow_tripdata_2024-01_clean.parquet\"\n",
    "\n",
    "# Save cleaned data file\n",
    "df.write_parquet(output_path)\n",
    "\n",
    "print(\"\\nCleaned data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b4b10c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x1f07e34a130>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DuckDB connection \n",
    "con = duckdb.connect()\n",
    "\n",
    "# Load the zones data into a Polars DataFrame\n",
    "zones = pl.read_csv(\"data/raw/taxi_zone_lookup.csv\")\n",
    "\n",
    "# Register the Polars DataFrames as a DuckDB table\n",
    "con.register(\"trips\", df.to_arrow())\n",
    "con.register(\"zones\", zones.to_arrow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0495f3fd",
   "metadata": {},
   "source": [
    "The following query shows the top 10 busiest zones by their total number of trips including that zone in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ad0ce8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Zone  total_trips\n",
      "0                Midtown Center       140161\n",
      "1         Upper East Side South       140134\n",
      "2                   JFK Airport       138478\n",
      "3         Upper East Side North       133975\n",
      "4                  Midtown East       104356\n",
      "5     Times Sq/Theatre District       102972\n",
      "6  Penn Station/Madison Sq West       102161\n",
      "7           Lincoln Square East       101800\n",
      "8             LaGuardia Airport        87715\n",
      "9         Upper West Side South        86475\n"
     ]
    }
   ],
   "source": [
    "busiest_pickup_zones = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        z.Zone,\n",
    "        COUNT(*) AS total_trips\n",
    "    FROM trips t\n",
    "    JOIN zones z\n",
    "    ON t.PULocationID = z.LocationID\n",
    "    GROUP BY z.Zone\n",
    "    ORDER BY total_trips DESC\n",
    "    LIMIT 10;\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "print(busiest_pickup_zones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5eb4dd",
   "metadata": {},
   "source": [
    "The following query shows the average fare amount paid for trips at every pickup hour of the day ordered by the hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5dde42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pickup_hour   avg_fare\n",
      "0             0  19.679250\n",
      "1             1  17.732032\n",
      "2             2  16.621723\n",
      "3             3  18.530033\n",
      "4             4  23.435229\n",
      "5             5  27.492713\n",
      "6             6  22.026585\n",
      "7             7  18.749879\n",
      "8             8  17.822939\n",
      "9             9  17.943989\n",
      "10           10  18.047523\n",
      "11           11  17.628112\n",
      "12           12  17.796520\n",
      "13           13  18.418805\n",
      "14           14  19.271523\n",
      "15           15  19.110366\n",
      "16           16  19.457290\n",
      "17           17  18.118545\n",
      "18           18  17.013712\n",
      "19           19  17.626564\n",
      "20           20  18.050403\n",
      "21           21  18.292862\n",
      "22           22  19.110051\n",
      "23           23  20.243498\n"
     ]
    }
   ],
   "source": [
    "avg_fare_hourly = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        pickup_hour,\n",
    "        AVG(fare_amount) AS avg_fare\n",
    "    FROM trips\n",
    "    GROUP BY pickup_hour\n",
    "    ORDER BY pickup_hour;\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "print(avg_fare_hourly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66849a32",
   "metadata": {},
   "source": [
    "The following query shows the percentage of total trips that were paid for by each payment type available descending by percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d61ce027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   payment_type  percentage\n",
      "0             1   80.081196\n",
      "1             2   14.735400\n",
      "2             0    4.015363\n",
      "3             4    0.797045\n",
      "4             3    0.370997\n"
     ]
    }
   ],
   "source": [
    "trips_by_payment = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        payment_type,\n",
    "        COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () AS percentage\n",
    "    FROM trips\n",
    "    GROUP BY payment_type\n",
    "    ORDER BY percentage DESC;\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "print(trips_by_payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32162f",
   "metadata": {},
   "source": [
    "The following query shows the average tip amount for each day of the week as percentage for only payments made with a credit card ordered by the day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "678ca237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pickup_day_of_week  avg_tip_percentage\n",
      "0                   1           25.513977\n",
      "1                   2           25.729989\n",
      "2                   3           25.706582\n",
      "3                   4           29.734458\n",
      "4                   5           25.595719\n",
      "5                   6           26.293897\n",
      "6                   7           25.100984\n"
     ]
    }
   ],
   "source": [
    "tip_percentage_card = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        pickup_day_of_week,\n",
    "        AVG(CASE WHEN fare_amount > 0 THEN tip_amount / fare_amount END) * 100 AS avg_tip_percentage\n",
    "    FROM trips\n",
    "    WHERE payment_type = 1\n",
    "    GROUP BY pickup_day_of_week\n",
    "    ORDER BY pickup_day_of_week;\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "print(tip_percentage_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42896870",
   "metadata": {},
   "source": [
    "The following query shows the top 5 most common pickup and dropoff zone pairs by the total amount of trips including both zones in descending order of trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "594b2790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pickup_zone           dropoff_zone  trip_count\n",
      "0  Upper East Side South  Upper East Side North       21642\n",
      "1  Upper East Side North  Upper East Side South       19199\n",
      "2  Upper East Side North  Upper East Side North       15200\n",
      "3  Upper East Side South  Upper East Side South       14119\n",
      "4         Midtown Center  Upper East Side South       10139\n"
     ]
    }
   ],
   "source": [
    "common_trip_routes = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        zp.Zone AS pickup_zone,\n",
    "        zd.Zone AS dropoff_zone,\n",
    "        COUNT(*) AS trip_count\n",
    "    FROM trips t\n",
    "    JOIN zones zp\n",
    "        ON t.PULocationID = zp.LocationID\n",
    "    JOIN zones zd\n",
    "        ON t.DOLocationID = zd.LocationID\n",
    "    GROUP BY pickup_zone, dropoff_zone\n",
    "    ORDER BY trip_count DESC\n",
    "    LIMIT 5;\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "print(common_trip_routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a495f1",
   "metadata": {},
   "source": [
    "Part 3: Dashboard Development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
