{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3423ca0f",
   "metadata": {},
   "source": [
    "Part 1: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec86fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Define URLs for required files\n",
    "taxi_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "zone_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "\n",
    "# Create data/raw directory if it doesn't exist\n",
    "data_dir = Path(\"data/raw\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Defines File paths for downloaded data\n",
    "taxi_path = data_dir / \"yellow_tripdata_2024-01.parquet\"\n",
    "zone_path = data_dir / \"taxi_zone_lookup.csv\"\n",
    "\n",
    "# Download Files and write to specified paths\n",
    "def download_file(url, path):\n",
    "     with requests.get(url, stream=True) as r:\n",
    "         r.raise_for_status()\n",
    "         with open(path, \"wb\") as f:\n",
    "             for chunk in r.iter_content(chunk_size=8192):\n",
    "                 f.write(chunk)\n",
    "\n",
    "download_file(taxi_url, taxi_path)\n",
    "download_file(zone_url, zone_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042768f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'df' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(df.dtypes)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mData Validated Successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m print_summary()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m():\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Check for missing columns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     missing_cols = \u001b[38;5;28mset\u001b[39m(columns) - \u001b[38;5;28mset\u001b[39m(\u001b[43mdf\u001b[49m.columns)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing expected columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'df' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Define expected columns\n",
    "columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \"DOLocationID\", \n",
    "           \"passenger_count\", \"trip_distance\", \"fare_amount\", \"tip_amount\", \"total_amount\",\n",
    "           \"payment_type\"]\n",
    "\n",
    "datetime_columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "\n",
    "# Load Data with Polars\n",
    "df = pl.read_parquet(taxi_path, columns=columns)\n",
    "\n",
    "def validate_data(df):\n",
    "    # Check for missing columns\n",
    "    missing_cols = set(columns) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise Exception(f\"Missing expected columns: {missing_cols}\")\n",
    "\n",
    "    # Validate datetime columns\n",
    "    for col in datetime_columns:\n",
    "        if not df[col].dtype == pl.Datetime:\n",
    "            try:\n",
    "                df = df.with_columns(pl.col(col).str.strptime(pl.Datetime, strict=False))\n",
    "            except Exception:\n",
    "                raise Exception(f\"Invalid datetime values detected in column: {col}\")\n",
    "    return df\n",
    "\n",
    "# Print Row Count and Summary\n",
    "def print_summary():\n",
    "    print(\"\\n=== Dataset Summary ===\")\n",
    "    print(f\"Total rows: {len(df):,}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nColumn Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nData Validated Successfully!\")\n",
    "\n",
    "validate_data(df)\n",
    "print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6e450",
   "metadata": {},
   "source": [
    "Part 2: Data Transformation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc638c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaned Dataset Summary ===\n",
      "Total rows removed: 94,522\n",
      "Removed null values: 0\n",
      "Removed invalid distances: 60,371\n",
      "Removed negative fares: 34,065\n",
      "Removed exceeding $500: 30\n",
      "Removed invalid times: 56\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with nulls\n",
    "def remove_nulls(df):\n",
    "    num_rows = len(df)\n",
    "\n",
    "    critical_columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \n",
    "                    \"DOLocationID\", \"fare_amount\"]\n",
    "    \n",
    "    df = df.drop_nulls(critical_columns)\n",
    "\n",
    "    removed_nulls = num_rows - len(df)\n",
    "    return df, removed_nulls\n",
    "\n",
    "# Filter out invalid trips tracking reasons for removal\n",
    "def filter_trips(df):\n",
    "    current_rows = len(df)\n",
    "\n",
    "    df = df.filter(pl.col(\"trip_distance\") > 0)\n",
    "    invalid_distance = current_rows - len(df)\n",
    "    current_rows = len(df)\n",
    "\n",
    "    df = df.filter(pl.col(\"fare_amount\") >= 0)\n",
    "    negative_fare = current_rows - len(df)\n",
    "    current_rows = len(df)\n",
    "\n",
    "    df = df.filter(pl.col(\"fare_amount\") <= 500)\n",
    "    exceeding_max = current_rows - len(df)\n",
    "\n",
    "    return df, invalid_distance, negative_fare, exceeding_max\n",
    "\n",
    "# Filter out trips with dropoff before pickup\n",
    "def filter_time(df):\n",
    "    num_rows = len(df)\n",
    "\n",
    "    df = df.filter(pl.col(\"tpep_dropoff_datetime\") >= pl.col(\"tpep_pickup_datetime\"))\n",
    "\n",
    "    removed_time = num_rows - len(df)\n",
    "    return df, removed_time\n",
    "\n",
    "# Save cleaned data and print summary of removals\n",
    "def save_and_print(df, total_removed, removed_nulls, invalid_distance, negative_fare, exceeding_max, removed_time):\n",
    "    print(\"\\n=== Cleaned Dataset Summary ===\")\n",
    "    print(f\"Total rows removed: {total_removed:,}\")\n",
    "    print(f\"Removed null values: {removed_nulls:,}\")\n",
    "    print(f\"Removed invalid distances: {invalid_distance:,}\")\n",
    "    print(f\"Removed negative fares: {negative_fare:,}\")\n",
    "    print(f\"Removed exceeding $500: {exceeding_max:,}\")\n",
    "    print(f\"Removed invalid times: {removed_time:,}\")\n",
    "\n",
    "original_rows = len(df)\n",
    "\n",
    "df, removed_nulls = remove_nulls(df)\n",
    "df, invalid_distance, negative_fare, exceeding_max = filter_trips(df)\n",
    "df, removed_time = filter_time(df)\n",
    "\n",
    "total_removed = original_rows - len(df)\n",
    "\n",
    "save_and_print(df, total_removed, removed_nulls, invalid_distance, negative_fare, exceeding_max, removed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_duration_minutes(df):\n",
    "    df = df.with_columns([\n",
    "        ((pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\")).dt.total_seconds() / 60)\n",
    "        .alias(\"trip_duration_minutes\")\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_derived_columns(df):\n",
    "    df = create_duration_minutes(df)\n",
    "\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"trip_distance\") / (pl.col(\"trip_duration_minutes\") / 60)).fill_null(0).alias(\"trip_speed_mph\"),\n",
    "\n",
    "        pl.col('tpep_pickup_datetime').dt.hour().alias('pickup_hour'),\n",
    "\n",
    "        pl.col('tpep_pickup_datetime').dt.weekday().alias('pickup_day_of_week')\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "        \n",
    "df = create_derived_columns(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
